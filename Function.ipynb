{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\deepanshu\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\deepanshu\\anaconda3\\lib\\site-packages (from bs4) (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in c:\\users\\deepanshu\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\deepanshu\\anaconda3\\lib\\site-packages (2.24.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\deepanshu\\anaconda3\\lib\\site-packages (from requests) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\deepanshu\\anaconda3\\lib\\site-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\deepanshu\\anaconda3\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\deepanshu\\anaconda3\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: html5lib in c:\\users\\deepanshu\\anaconda3\\lib\\site-packages (1.1)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\deepanshu\\anaconda3\\lib\\site-packages (from html5lib) (1.15.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\deepanshu\\anaconda3\\lib\\site-packages (from html5lib) (0.5.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement urllib (from versions: none)\n",
      "ERROR: No matching distribution found for urllib\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install requests\n",
    "!pip install html5lib\n",
    "import bs4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import webbrowser\n",
    "import  os\n",
    "import nltk\n",
    "!pip install urllib\n",
    "import urllib.request\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading The Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel(\"D:\\Folder\\Projects Python\\Assignment\\cik_list.xlsx\")\n",
    "data=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Master_Dict=pd.read_excel(\"D:\\Folder\\Projects Python\\Assignment\\Master_Dict.xlsx\")\n",
    "Constrain_D=pd.read_excel(\"D:\\Folder\\Projects Python\\Assignment\\constraining_dictionary.xlsx\")\n",
    "Unconstrain_D=pd.read_excel(\"D:\\\\Folder\\\\Projects Python\\\\Assignment\\\\uncertainty_dictionary.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [file for file in os.listdir(\"D:\\Folder\\Projects Python\\Assignment\\stopwords\")]\n",
    "file1=open(\"D:\\Folder\\Projects Python\\Assignment\\\\new_file.txt\",'w')\n",
    "\n",
    "for file in files:\n",
    "    SW = open(\"D:\\Folder\\Projects Python\\Assignment\\stopwords\\\\\" + file)\n",
    "    SW_read=SW.read()\n",
    "    file1.write(SW_read.upper())\n",
    "    \n",
    "file1.close()\n",
    "file2=open(\"D:\\Folder\\Projects Python\\Assignment\\\\new_file.txt\" , \"r\")\n",
    "\n",
    "stopword=file2.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_define(k):\n",
    "\n",
    "        url='https://www.seci/Archives/'+data['SECFNAME'][k]\n",
    "        \n",
    "        return(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting and Cleaning of Data From Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\DEEPANSHU\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "from urllib.request import urlopen, Request\n",
    "finalp=[]\n",
    "\n",
    "def clean_text(i):\n",
    "    final_list=[]\n",
    "    new_line_list=[]\n",
    "    list1=[]\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.3'}\n",
    "    reg_url = i\n",
    "    req = Request(url=reg_url, headers=headers) \n",
    "    html = urlopen(req).read()\n",
    "    decoded_line = html.decode(\"utf-8\")\n",
    "    cleaning = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleaning, '', decoded_line)\n",
    "    new_line=re.sub(('[^A-Z a-z.]'),\" \",cleantext)        \n",
    "    new_line_list.append(new_line)    \n",
    "    \n",
    "    final=\" \".join(w for w in nltk.wordpunct_tokenize(new_line)               if w.lower() in words )\n",
    "    final_list.append(final)\n",
    "        \n",
    "    complete=re.sub(r'\\n\\s*\\n','\\n',final,re.MULTILINE)\n",
    "    list1.append(complete)\n",
    "    newlist = (item.strip() if hasattr(item, 'strip') else item for item in list1)\n",
    "    \n",
    "    list5=[item for item in newlist if item != '']\n",
    "    finalp.append(list5)\n",
    "    return(list5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning From Stopwords and Variable 8: Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopword_clean1(cleaned_list1):\n",
    "    new=[]\n",
    "    single_occurance_word_list=[]\n",
    "    \n",
    "    temp= cleaned_list1.upper().split()\n",
    "    val=[new.append(j) for j in temp if not j in stopword] \n",
    "        \n",
    "    for k in new:\n",
    "        if k not in single_occurance_word_list:\n",
    "            single_occurance_word_list.append(k)\n",
    "                \n",
    "    word_count=len(new)\n",
    "    return(single_occurance_word_list, word_count) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Functions for Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable 1, 2 and 3 : Positive Score, Negative Score and Polarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Variable_1_2_3(single_occurance_word_list):\n",
    "   \n",
    "    Final_Master_Dict=pd.concat([Master_Dict[Master_Dict['Negative']!=0] , Master_Dict[Master_Dict['Positive']!=0]],axis=0)[['Word','Positive','Negative']]\n",
    "    PS=0\n",
    "    NS=0\n",
    "    \n",
    "    for i in range(0,len(single_occurance_word_list)):\n",
    "        for j in Final_Master_Dict.index:\n",
    "            if single_occurance_word_list[i].upper()==Final_Master_Dict['Word'][j]:\n",
    "                \n",
    "                if Final_Master_Dict['Positive'][j]!=0:\n",
    "                    PS=PS+1\n",
    "                else:\n",
    "                    NS=NS-1\n",
    "                    \n",
    "    Polarity_Score= (PS-NS*(-1))/((PS + NS*(-1))+0.000001)\n",
    "\n",
    "    return (PS,NS*(-1),Polarity_Score)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable 4, 5, 6 and 7: Average Sentence Length, Percentage of complex words, Fog Index and Complex Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphen import Pyphen\n",
    "def variable_4_5_6_7(clean_text_list):\n",
    "    Avg_sentence_length=[]\n",
    "    complex_words_percent=[]\n",
    "    FogIndex=[]\n",
    "    decoded=[]\n",
    "    Complex_Words_Count_list=[]\n",
    "    for i in data.index:\n",
    "        for cleaned_list1 in clean_text_list[i]:\n",
    "            url='https://www.seci/Archives/'+data['SECFNAME'][i]\n",
    "            Complex_Words_Count=0\n",
    "            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.3'}\n",
    "            reg_url = url\n",
    "            req = Request(url=reg_url, headers=headers) \n",
    "            html = urlopen(req).read()\n",
    "            decoded_line = html.decode(\"utf-8\")\n",
    "            new_line=re.sub(('[^A-Z a-z.]'),\" \",decoded_line)\n",
    "            decoded.append(new_line)\n",
    "            CWC=0\n",
    "                    \n",
    "            for l in decoded:\n",
    "                val=l.split()\n",
    "        \n",
    "                for o in val:\n",
    "                    p = Pyphen(lang='en_US')\n",
    "                    \n",
    "            \n",
    "                    for lo in range(0,len((p.positions(o)))):\n",
    "                        \n",
    "                    \n",
    "                        if len(p.positions(o)) > 2:\n",
    "                    \n",
    "                            CWC=CWC+1\n",
    "        print(i)\n",
    "            \n",
    "           \n",
    "                            \n",
    "        ASL=len(val)/len(cleaned_list1)\n",
    "        CWP=(CWC)/(len(val))   \n",
    "        FI = 0.4 * (ASL + CWP)\n",
    "        Avg_sentence_length.append(ASL)\n",
    "        complex_words_percent.append(CWP)\n",
    "        FogIndex.append(FI)\n",
    "        Complex_Words_Count_list.append(CWC)\n",
    "        \n",
    "        \n",
    "    \n",
    "    return(Avg_sentence_length,complex_words_percent, FogIndex, Complex_Words_Count_list)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable 9 and 10: Uncertainty and Constraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_9_10(single_occurance_word_list):\n",
    "    \n",
    "    Constraining=0\n",
    "    Uncertainty=0\n",
    "\n",
    "    for i in range(0,len(single_occurance_word_list)):\n",
    "        for j in Constrain_D.index:\n",
    "            if single_occurance_word_list[i].upper()==Constrain_D['Word'][j]:\n",
    "                Constraining=Constraining+1\n",
    "                \n",
    "        for k in Unconstrain_D.index:\n",
    "            if single_occurance_word_list[i].upper()==Unconstrain_D['Word'][k]:\n",
    "                Uncertainty=Uncertainty+1\n",
    "                \n",
    "    return(Uncertainty,Constraining )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable 11,12,13 and 14: Positive word proportion, Negative word proportion, Uncertainty word proportion and Constraining word proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_11_12_13_4(PS,NS,Uncertainty,Constraining,word_count):\n",
    "\n",
    "    PWP=PS/word_count\n",
    "    NWP=NS/word_count\n",
    "    UWP=Uncertainty/word_count\n",
    "    CWP=Constraining/word_count\n",
    "    \n",
    "    return (PWP,NWP,UWP,CWP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varible 15: Constraining words for whole report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def variable_15(cleaned_list1):\n",
    "    CWP=0\n",
    "    \n",
    "    for i in cleaned_list1.split():\n",
    "        for s in Constrain_D.index:\n",
    "            if i.upper()==Constrain_D['Word'][s]:\n",
    "                CWP=CWP+1  \n",
    "                \n",
    "    return(CWP)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
